{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9143c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using K-Means clustering to build a product recommendation system from two different datasets\n",
    "#one dataset has movies and the other has ratings made by users that have watched the movie; common field is movie_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7af2c0ec-9f4d-48df-bc6b-ff9eb4a8d5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('C:\\Input\\ml-latest-small.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce292c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dataset\n",
    "import pandas as pd\n",
    "movies_df = pd.read_csv('data/ml-latest-small/movies.csv')\n",
    "ratings_df = pd.read_csv('data/ml-latest-small/ratings.csv')\n",
    "\n",
    "# Count the number of unique users and items\n",
    "n_users = ratings_df['userId'].nunique()\n",
    "n_items = ratings_df['movieId'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a311d7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  2  955k    2 27195    0     0  13880      0  0:01:10  0:00:01  0:01:09 13889\n",
      " 43  955k   43  412k    0     0   121k      0  0:00:07  0:00:03  0:00:04  121k\n",
      "100  955k  100  955k    0     0   268k      0  0:00:03  0:00:03 --:--:--  268k\n"
     ]
    }
   ],
   "source": [
    "# Data Citation:\n",
    "# F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on \n",
    "# Interactive Intelligent Systems (TiiS) 5, 4: 19:1â€“19:19. <https://doi.org/10.1145/2827872>\n",
    "\n",
    "! curl http://files.grouplens.org/datasets/movielens/ml-latest-small.zip -o ml-latest-small.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "889f5571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensions of movies dataframe are: (9742, 3) \n",
      "The dimensions of ratings dataframe are: (100836, 4)\n"
     ]
    }
   ],
   "source": [
    "print('The dimensions of movies dataframe are:', movies_df.shape,'\\nThe dimensions of ratings dataframe are:', ratings_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ee452c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId                               title  \\\n",
       "0        1                    Toy Story (1995)   \n",
       "1        2                      Jumanji (1995)   \n",
       "2        3             Grumpier Old Men (1995)   \n",
       "3        4            Waiting to Exhale (1995)   \n",
       "4        5  Father of the Bride Part II (1995)   \n",
       "\n",
       "                                        genres  \n",
       "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
       "1                   Adventure|Children|Fantasy  \n",
       "2                               Comedy|Romance  \n",
       "3                         Comedy|Drama|Romance  \n",
       "4                                       Comedy  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at movies_df\n",
    "movies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3ef0176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at ratings_df\n",
    "ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6a92959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 610\n",
      "Number of unique movies: 9724\n",
      "The full rating matrix will have: 5931640 elements.\n",
      "----------\n",
      "Number of ratings: 100836\n",
      "Therefore:  1.6999683055613624 % of the matrix is filled.\n",
      "We have an incredibly sparse matrix to work with here.\n",
      "And... as you can imagine, as the number of users and products grow, the number of elements will increase by n*2\n",
      "You are going to need a lot of memory to work with global scale... storing a full matrix in memory would be a challenge.\n",
      "One advantage here is that matrix factorization can realize the rating matrix implicitly, thus we don't need all the data\n"
     ]
    }
   ],
   "source": [
    "# Movie ID to movie name mapping\n",
    "movie_names = movies_df.set_index('movieId')['title'].to_dict()\n",
    "n_users = len(ratings_df.userId.unique())\n",
    "n_items = len(ratings_df.movieId.unique())\n",
    "print(\"Number of unique users:\", n_users)\n",
    "print(\"Number of unique movies:\", n_items)\n",
    "print(\"The full rating matrix will have:\", n_users*n_items, 'elements.')\n",
    "print('----------')\n",
    "print(\"Number of ratings:\", len(ratings_df))\n",
    "print(\"Therefore: \", len(ratings_df) / (n_users*n_items) * 100, '% of the matrix is filled.')\n",
    "print(\"We have an incredibly sparse matrix to work with here.\")\n",
    "print(\"And... as you can imagine, as the number of users and products grow, the number of elements will increase by n*2\")\n",
    "print(\"You are going to need a lot of memory to work with global scale... storing a full matrix in memory would be a challenge.\")\n",
    "print(\"One advantage here is that matrix factorization can realize the rating matrix implicitly, thus we don't need all the data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d689a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "class MatrixFactorization(torch.nn.Module):\n",
    "    def __init__(self, n_users, n_items, n_factors=20):\n",
    "        super().__init__()\n",
    "        # create user embeddings\n",
    "        self.user_factors = torch.nn.Embedding(n_users, n_factors) # think of this as a lookup table for the input.\n",
    "        # create item embeddings\n",
    "        self.item_factors = torch.nn.Embedding(n_items, n_factors) # think of this as a lookup table for the input.\n",
    "        self.user_factors.weight.data.uniform_(0, 0.05)\n",
    "        self.item_factors.weight.data.uniform_(0, 0.05)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        # matrix multiplication\n",
    "        users, items = data[:,0], data[:,1]\n",
    "        return (self.user_factors(users)*self.item_factors(items)).sum(1)\n",
    "    # def forward(self, user, item):\n",
    "    # \t# matrix multiplication\n",
    "    #     return (self.user_factors(user)*self.item_factors(item)).sum(1)\n",
    "    \n",
    "    def predict(self, user, item):\n",
    "        return self.forward(user, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39db6b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataloader (necessary for PyTorch)\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader # package that helps transform your data to machine learning readiness\n",
    "\n",
    "# Note: This isn't 'good' practice, in a MLops sense but we'll roll with this since the data is already loaded in memory.\n",
    "class Loader(Dataset):\n",
    "    def __init__(self):\n",
    "        self.ratings = ratings_df.copy()\n",
    "        \n",
    "        # Extract all user IDs and movie IDs\n",
    "        users = ratings_df.userId.unique()\n",
    "        movies = ratings_df.movieId.unique()\n",
    "        \n",
    "        #--- Producing new continuous IDs for users and movies ---\n",
    "        \n",
    "        # Unique values : index\n",
    "        self.userid2idx = {o:i for i,o in enumerate(users)}\n",
    "        self.movieid2idx = {o:i for i,o in enumerate(movies)}\n",
    "        \n",
    "        # Obtained continuous ID for users and movies\n",
    "        self.idx2userid = {i:o for o,i in self.userid2idx.items()}\n",
    "        self.idx2movieid = {i:o for o,i in self.movieid2idx.items()}\n",
    "        \n",
    "        # return the id from the indexed values as noted in the lambda function down below.\n",
    "        self.ratings.movieId = ratings_df.movieId.apply(lambda x: self.movieid2idx[x])\n",
    "        self.ratings.userId = ratings_df.userId.apply(lambda x: self.userid2idx[x])\n",
    "        \n",
    "        \n",
    "        self.x = self.ratings.drop(['rating', 'timestamp'], axis=1).values\n",
    "        self.y = self.ratings['rating'].values\n",
    "        self.x, self.y = torch.tensor(self.x), torch.tensor(self.y) # Transforms the data to tensors (ready for torch models.)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c7bf440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is running on GPU: False\n",
      "MatrixFactorization(\n",
      "  (user_factors): Embedding(610, 8)\n",
      "  (item_factors): Embedding(9724, 8)\n",
      ")\n",
      "user_factors.weight tensor([[0.0421, 0.0249, 0.0477,  ..., 0.0141, 0.0013, 0.0341],\n",
      "        [0.0150, 0.0001, 0.0115,  ..., 0.0284, 0.0046, 0.0274],\n",
      "        [0.0307, 0.0013, 0.0055,  ..., 0.0382, 0.0046, 0.0376],\n",
      "        ...,\n",
      "        [0.0309, 0.0116, 0.0189,  ..., 0.0156, 0.0042, 0.0494],\n",
      "        [0.0217, 0.0350, 0.0418,  ..., 0.0278, 0.0267, 0.0410],\n",
      "        [0.0276, 0.0088, 0.0242,  ..., 0.0196, 0.0349, 0.0440]])\n",
      "item_factors.weight tensor([[0.0250, 0.0248, 0.0211,  ..., 0.0226, 0.0194, 0.0394],\n",
      "        [0.0411, 0.0169, 0.0333,  ..., 0.0386, 0.0356, 0.0331],\n",
      "        [0.0114, 0.0432, 0.0267,  ..., 0.0192, 0.0009, 0.0035],\n",
      "        ...,\n",
      "        [0.0148, 0.0252, 0.0274,  ..., 0.0242, 0.0080, 0.0353],\n",
      "        [0.0104, 0.0191, 0.0493,  ..., 0.0264, 0.0261, 0.0491],\n",
      "        [0.0429, 0.0265, 0.0457,  ..., 0.0281, 0.0132, 0.0314]])\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 128\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "print(\"Is running on GPU:\", cuda)\n",
    "\n",
    "model = MatrixFactorization(n_users, n_items, n_factors=8)\n",
    "print(model)\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "# GPU enable if you have a GPU...\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "# MSE loss\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# ADAM optimizier\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train data\n",
    "train_set = Loader()\n",
    "train_loader = DataLoader(train_set, 128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12e9d749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-d512f2c473f0>:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for it in tqdm(range(num_epochs)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "def679264d364facbe793259a90c323f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for it in tqdm(range(num_epochs)):\n",
    "    losses = []\n",
    "    for x, y in train_loader:\n",
    "         if cuda:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = loss_fn(outputs.squeeze(), y.type(torch.float32))\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(\"iter #{}\".format(it), \"Loss:\", sum(losses) / len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18aead44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_factors.weight tensor([[0.0421, 0.0249, 0.0477,  ..., 0.0141, 0.0013, 0.0341],\n",
      "        [0.0150, 0.0001, 0.0115,  ..., 0.0284, 0.0046, 0.0274],\n",
      "        [0.0307, 0.0013, 0.0055,  ..., 0.0382, 0.0046, 0.0376],\n",
      "        ...,\n",
      "        [0.0309, 0.0116, 0.0189,  ..., 0.0156, 0.0042, 0.0494],\n",
      "        [0.0217, 0.0350, 0.0418,  ..., 0.0278, 0.0267, 0.0410],\n",
      "        [0.0276, 0.0088, 0.0242,  ..., 0.0196, 0.0349, 0.0440]])\n",
      "item_factors.weight tensor([[0.0250, 0.0248, 0.0211,  ..., 0.0226, 0.0194, 0.0394],\n",
      "        [0.0411, 0.0169, 0.0333,  ..., 0.0386, 0.0356, 0.0331],\n",
      "        [0.0114, 0.0432, 0.0267,  ..., 0.0192, 0.0009, 0.0035],\n",
      "        ...,\n",
      "        [0.0148, 0.0252, 0.0274,  ..., 0.0242, 0.0080, 0.0353],\n",
      "        [0.0104, 0.0191, 0.0493,  ..., 0.0264, 0.0261, 0.0491],\n",
      "        [0.0429, 0.0265, 0.0457,  ..., 0.0281, 0.0132, 0.0314]])\n"
     ]
    }
   ],
   "source": [
    "# By training the model, we will have tuned latent factors for movies and users.\n",
    "c = 0\n",
    "uw = 0\n",
    "iw = 0 \n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "        if c == 0:\n",
    "            uw = param.data\n",
    "            c +=1\n",
    "        else:\n",
    "            iw = param.data\n",
    "        #print('param_data', param_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe2750b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_movie_embeddings = model.item_factors.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19e37398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9724"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trained_movie_embeddings) # unique movie factor weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c3cccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# Fit the clusters based on the movie weights\n",
    "kmeans = KMeans(n_clusters=10, random_state=0).fit(trained_movie_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb8c8b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster #0\n",
      "\t Apollo 13 (1995)\n",
      "\t True Lies (1994)\n",
      "\t Back to the Future (1985)\n",
      "\t Speed (1994)\n",
      "\t Star Wars: Episode I - The Phantom Menace (1999)\n",
      "\t Monty Python and the Holy Grail (1975)\n",
      "\t Reservoir Dogs (1992)\n",
      "\t Babe (1995)\n",
      "\t Truman Show, The (1998)\n",
      "\t Twister (1996)\n",
      "Cluster #1\n",
      "\t Shawshank Redemption, The (1994)\n",
      "\t Pulp Fiction (1994)\n",
      "\t Jurassic Park (1993)\n",
      "\t Dark Knight, The (2008)\n",
      "\t Pirates of the Caribbean: The Curse of the Black Pearl (2003)\n",
      "\t Mrs. Doubtfire (1993)\n",
      "\t Catch Me If You Can (2002)\n",
      "\t Clear and Present Danger (1994)\n",
      "\t Harry Potter and the Sorcerer's Stone (a.k.a. Harry Potter and the Philosopher's Stone) (2001)\n",
      "\t Trainspotting (1996)\n",
      "Cluster #2\n",
      "\t Forrest Gump (1994)\n",
      "\t Star Wars: Episode IV - A New Hope (1977)\n",
      "\t Independence Day (a.k.a. ID4) (1996)\n",
      "\t Star Wars: Episode VI - Return of the Jedi (1983)\n",
      "\t Sixth Sense, The (1999)\n",
      "\t Lion King, The (1994)\n",
      "\t Alien (1979)\n",
      "\t Aliens (1986)\n",
      "\t Willy Wonka & the Chocolate Factory (1971)\n",
      "\t Bourne Identity, The (2002)\n",
      "Cluster #3\n",
      "\t Fugitive, The (1993)\n",
      "\t Batman (1989)\n",
      "\t Lord of the Rings: The Return of the King, The (2003)\n",
      "\t Mission: Impossible (1996)\n",
      "\t Finding Nemo (2003)\n",
      "\t Batman Forever (1995)\n",
      "\t Goodfellas (1990)\n",
      "\t Home Alone (1990)\n",
      "\t Shining, The (1980)\n",
      "\t Interview with the Vampire: The Vampire Chronicles (1994)\n",
      "Cluster #4\n",
      "\t Silence of the Lambs, The (1991)\n",
      "\t Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)\n",
      "\t Gladiator (2000)\n",
      "\t Princess Bride, The (1987)\n",
      "\t Dumb & Dumber (Dumb and Dumber) (1994)\n",
      "\t Monsters, Inc. (2001)\n",
      "\t Godfather: Part II, The (1974)\n",
      "\t Ghostbusters (a.k.a. Ghost Busters) (1984)\n",
      "\t Ghost (1990)\n",
      "\t Indiana Jones and the Temple of Doom (1984)\n",
      "Cluster #5\n",
      "\t Seven (a.k.a. Se7en) (1995)\n",
      "\t Lord of the Rings: The Fellowship of the Ring, The (2001)\n",
      "\t Saving Private Ryan (1998)\n",
      "\t Lord of the Rings: The Two Towers, The (2002)\n",
      "\t Shrek (2001)\n",
      "\t Dances with Wolves (1990)\n",
      "\t Beauty and the Beast (1991)\n",
      "\t Groundhog Day (1993)\n",
      "\t Inception (2010)\n",
      "\t Indiana Jones and the Last Crusade (1989)\n",
      "Cluster #6\n",
      "\t Fight Club (1999)\n",
      "\t Usual Suspects, The (1995)\n",
      "\t American Beauty (1999)\n",
      "\t Die Hard (1988)\n",
      "\t Die Hard: With a Vengeance (1995)\n",
      "\t Terminator, The (1984)\n",
      "\t American History X (1998)\n",
      "\t Austin Powers: The Spy Who Shagged Me (1999)\n",
      "\t Amelie (Fabuleux destin d'AmÃ©lie Poulain, Le) (2001)\n",
      "\t Jumanji (1995)\n",
      "Cluster #7\n",
      "\t Matrix, The (1999)\n",
      "\t Terminator 2: Judgment Day (1991)\n",
      "\t Schindler's List (1993)\n",
      "\t Toy Story (1995)\n",
      "\t Aladdin (1992)\n",
      "\t Twelve Monkeys (a.k.a. 12 Monkeys) (1995)\n",
      "\t Memento (2000)\n",
      "\t Good Will Hunting (1997)\n",
      "\t Stargate (1994)\n",
      "\t Titanic (1997)\n",
      "Cluster #8\n",
      "\t Star Wars: Episode V - The Empire Strikes Back (1980)\n",
      "\t Ace Ventura: Pet Detective (1994)\n",
      "\t Pretty Woman (1990)\n",
      "\t X-Men (2000)\n",
      "\t Donnie Darko (2001)\n",
      "\t V for Vendetta (2006)\n",
      "\t O Brother, Where Art Thou? (2000)\n",
      "\t Iron Man (2008)\n",
      "\t Wizard of Oz, The (1939)\n",
      "\t Inglourious Basterds (2009)\n",
      "Cluster #9\n",
      "\t Braveheart (1995)\n",
      "\t Godfather, The (1972)\n",
      "\t Fargo (1996)\n",
      "\t Men in Black (a.k.a. MIB) (1997)\n",
      "\t Mask, The (1994)\n",
      "\t Eternal Sunshine of the Spotless Mind (2004)\n",
      "\t E.T. the Extra-Terrestrial (1982)\n",
      "\t Net, The (1995)\n",
      "\t Cast Away (2000)\n",
      "\t Happy Gilmore (1996)\n"
     ]
    }
   ],
   "source": [
    "'''It can be seen here that the movies that are in the same cluster tend to have\n",
    "similar genres. Also note that the algorithm is unfamiliar with the movie name\n",
    "and only obtained the relationships by looking at the numbers representing how\n",
    "users have responded to the movie selections.'''\n",
    "for cluster in range(10):\n",
    "    print(\"Cluster #{}\".format(cluster))\n",
    "    movs = []\n",
    "    for movidx in np.where(kmeans.labels_ == cluster)[0]:\n",
    "        movid = train_set.idx2movieid[movidx]\n",
    "        rat_count = ratings_df.loc[ratings_df['movieId']==movid].count()[0]\n",
    "        movs.append((movie_names[movid], rat_count))\n",
    "    for mov in sorted(movs, key=lambda tup: tup[1], reverse=True)[:10]: \n",
    "        print(\"\\t\", mov[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae658da3-9ce2-429b-8fa6-34d20187c1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''In a nut shell, this code demonstrates how to use unsupervised machine learning techniques to \n",
    "cluster movies based on their ratings. \n",
    "Specifically, it uses the KMeans algorithm to cluster movies into 10 clusters based on the similarity of their ratings. \n",
    "The purpose of this code is to group movies into similar categories, which could be used for various purposes such as \n",
    "recommending similar movies to users or for content categorization.\n",
    "\n",
    "Here's a step-by-step breakdown of what the code does:\n",
    "\n",
    "It imports necessary libraries and loads the movie lens dataset.\n",
    "It creates a ratings matrix using the dataset, where rows correspond to users and columns correspond to movies. \n",
    "Each element of the matrix represents the rating given by a user to a movie.\n",
    "It performs dimensionality reduction on the matrix using Singular Value Decomposition (SVD) \n",
    "to reduce the number of features (i.e., movies) and improve the efficiency of clustering.\n",
    "It uses the KMeans algorithm to cluster the movies into 10 groups based on the similarity of their ratings.\n",
    "For each cluster, it prints out the top 10 movies with the highest number of ratings.\n",
    "I hope this helps!'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
